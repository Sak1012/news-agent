{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Agent Debug Notebook\n",
        "\n",
        "This notebook mirrors the Python modules so you can tinker with the agent, adjust providers, and try out queries interactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Dict, Iterable, List, Mapping, Optional\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import feedparser\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _split_csv(value: Optional[str]) -> List[str]:\n",
        "    if not value:\n",
        "        return []\n",
        "    return [item.strip() for item in value.split(\",\") if item.strip()]\n",
        "\n",
        "\n",
        "@dataclass(slots=True)\n",
        "class AgentConfig:\n",
        "    \"\"\"Runtime configuration for the news agent.\"\"\"\n",
        "\n",
        "    newsapi_key: Optional[str] = None\n",
        "    default_limit: int = 10\n",
        "    allowed_domains: List[str] = field(default_factory=list)\n",
        "\n",
        "    @classmethod\n",
        "    def from_env(cls) -> \"AgentConfig\":\n",
        "        return cls(\n",
        "            newsapi_key=os.getenv(\"NEWSAPI_KEY\"),\n",
        "            default_limit=int(os.getenv(\"NEWS_AGENT_DEFAULT_LIMIT\", \"10\")),\n",
        "            allowed_domains=_split_csv(os.getenv(\"NEWS_AGENT_ALLOWED_DOMAINS\")),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(slots=True)\n",
        "class RawArticle:\n",
        "    \"\"\"Raw article data collected from a provider.\"\"\"\n",
        "\n",
        "    title: str\n",
        "    url: str\n",
        "    source: str\n",
        "    published_at: Optional[datetime]\n",
        "    content: Optional[str]\n",
        "    description: Optional[str]\n",
        "\n",
        "\n",
        "@dataclass(slots=True)\n",
        "class NewsItem:\n",
        "    \"\"\"Structured representation of a processed article.\"\"\"\n",
        "\n",
        "    title: str\n",
        "    url: str\n",
        "    source: str\n",
        "    published_at: Optional[datetime]\n",
        "    summary: Optional[str]\n",
        "    sentiment: str\n",
        "    sentiment_score: float\n",
        "    excerpt: Optional[str] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "\n",
        "def summarize(text: Optional[str], max_sentences: int = 2) -> Optional[str]:\n",
        "    if not text:\n",
        "        return None\n",
        "    sentences = _split_sentences(text)\n",
        "    if not sentences:\n",
        "        return None\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \" \".join(sentences)\n",
        "    scores = _score_sentences(sentences)\n",
        "    ranked = sorted(enumerate(sentences), key=lambda item: scores.get(item[0], 0.0), reverse=True)\n",
        "    top_indices = sorted(idx for idx, _ in ranked[:max_sentences])\n",
        "    return \" \".join(sentences[idx] for idx in top_indices)\n",
        "\n",
        "\n",
        "def _split_sentences(text: str) -> List[str]:\n",
        "    split = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
        "    return [sentence.strip() for sentence in split if sentence.strip()]\n",
        "\n",
        "\n",
        "def _score_sentences(sentences: Iterable[str]) -> dict[int, float]:\n",
        "    words = [word.lower() for sentence in sentences for word in WORD_RE.findall(sentence)]\n",
        "    if not words:\n",
        "        return {}\n",
        "    freq = Counter(words)\n",
        "    max_freq = max(freq.values())\n",
        "    normalized = {word: count / max_freq for word, count in freq.items()}\n",
        "    scores: dict[int, float] = {}\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        tokens = WORD_RE.findall(sentence)\n",
        "        if not tokens:\n",
        "            continue\n",
        "        scores[idx] = sum(normalized.get(word.lower(), 0.0) for word in tokens) / len(tokens)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "POSITIVE_TOKENS = {\n",
        "    \"growth\",\n",
        "    \"improve\",\n",
        "    \"improving\",\n",
        "    \"surge\",\n",
        "    \"strong\",\n",
        "    \"beat\",\n",
        "    \"record\",\n",
        "    \"gain\",\n",
        "    \"positive\",\n",
        "    \"optimistic\",\n",
        "    \"upbeat\",\n",
        "    \"increase\",\n",
        "    \"exceed\",\n",
        "    \"sustainable\",\n",
        "    \"sustainability\",\n",
        "    \"expansion\",\n",
        "}\n",
        "\n",
        "NEGATIVE_TOKENS = {\n",
        "    \"loss\",\n",
        "    \"decline\",\n",
        "    \"drop\",\n",
        "    \"warning\",\n",
        "    \"weak\",\n",
        "    \"downturn\",\n",
        "    \"concern\",\n",
        "    \"miss\",\n",
        "    \"lawsuit\",\n",
        "    \"negative\",\n",
        "    \"risk\",\n",
        "    \"regulatory\",\n",
        "    \"penalty\",\n",
        "    \"fraud\",\n",
        "    \"downgrade\",\n",
        "}\n",
        "\n",
        "\n",
        "def score_sentiment(text: Optional[str]) -> tuple[str, float]:\n",
        "    if not text:\n",
        "        return \"neutral\", 0.0\n",
        "    lowered = text.lower()\n",
        "    pos_hits = sum(lowered.count(token) for token in POSITIVE_TOKENS)\n",
        "    neg_hits = sum(lowered.count(token) for token in NEGATIVE_TOKENS)\n",
        "    total = pos_hits + neg_hits\n",
        "    if total == 0:\n",
        "        return \"neutral\", 0.0\n",
        "    score = (pos_hits - neg_hits) / max(total, 1)\n",
        "    if score > 0.2:\n",
        "        return \"positive\", score\n",
        "    if score < -0.2:\n",
        "        return \"negative\", score\n",
        "    return \"neutral\", score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseProvider:\n",
        "    \"\"\"Abstract base class for content providers.\"\"\"\n",
        "\n",
        "    def fetch(self, query: str, limit: int = 10, **kwargs: Mapping[str, object]) -> Iterable[RawArticle]:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsAPIProvider(BaseProvider):\n",
        "    \"\"\"Fetch articles from newsapi.org when an API key is available.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "    def __init__(self, api_key: str) -> None:\n",
        "        if not api_key:\n",
        "            raise ValueError(\"NewsAPIProvider requires an API key\")\n",
        "        self._api_key = api_key\n",
        "\n",
        "    def fetch(self, query: str, limit: int = 10, **kwargs: Mapping[str, object]) -> Iterable[RawArticle]:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"pageSize\": limit,\n",
        "            \"language\": kwargs.get(\"language\", \"en\"),\n",
        "            \"sortBy\": kwargs.get(\"sort_by\", \"publishedAt\"),\n",
        "        }\n",
        "        response = requests.get(\n",
        "            self.BASE_URL,\n",
        "            params=params,\n",
        "            headers={\"Authorization\": self._api_key},\n",
        "            timeout=10,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        payload = response.json()\n",
        "        for article in payload.get(\"articles\", []):\n",
        "            yield RawArticle(\n",
        "                title=article.get(\"title\") or \"Untitled\",\n",
        "                url=article.get(\"url\") or \"\",\n",
        "                source=(article.get(\"source\") or {}).get(\"name\") or \"Unknown\",\n",
        "                published_at=_parse_date(article.get(\"publishedAt\")),\n",
        "                content=article.get(\"content\"),\n",
        "                description=article.get(\"description\"),\n",
        "            )\n",
        "\n",
        "\n",
        "def _parse_date(value: Optional[str]) -> Optional[datetime]:\n",
        "    if not value:\n",
        "        return None\n",
        "    try:\n",
        "        return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\"))\n",
        "    except ValueError:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WiredRSSProvider(BaseProvider):\n",
        "    \"\"\"Fetches and filters articles from Wired RSS feeds.\"\"\"\n",
        "\n",
        "    DEFAULT_SECTIONS = {\n",
        "        \"business\": \"https://www.wired.com/feed/category/business/latest/rss\",\n",
        "        \"science\": \"https://www.wired.com/feed/category/science/latest/rss\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, sections: Mapping[str, str] | None = None) -> None:\n",
        "        self._sections = dict(sections or self.DEFAULT_SECTIONS)\n",
        "\n",
        "    def fetch(self, query: str, limit: int = 10, **kwargs: Mapping[str, object]) -> Iterable[RawArticle]:\n",
        "        normalized_query = query.lower()\n",
        "        results: List[RawArticle] = []\n",
        "        for section, url in self._sections.items():\n",
        "            try:\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "            except Exception:\n",
        "                continue\n",
        "            feed = feedparser.parse(response.content)\n",
        "            for entry in feed.entries or []:\n",
        "                if normalized_query not in _entry_text(entry):\n",
        "                    continue\n",
        "                results.append(\n",
        "                    RawArticle(\n",
        "                        title=entry.get(\"title\") or f\"Wired {section.title()} Update\",\n",
        "                        url=entry.get(\"link\") or \"\",\n",
        "                        source=f\"Wired {section.title()}\",\n",
        "                        published_at=_parse_published(entry),\n",
        "                        content=_get_content(entry),\n",
        "                        description=entry.get(\"summary\"),\n",
        "                    )\n",
        "                )\n",
        "                if len(results) >= limit:\n",
        "                    return results\n",
        "        return results\n",
        "\n",
        "\n",
        "def _entry_text(entry: Mapping[str, object]) -> str:\n",
        "    title = str(entry.get(\"title\", \"\"))\n",
        "    summary = str(entry.get(\"summary\", \"\"))\n",
        "    content = \"\"\n",
        "    contents = entry.get(\"content\")\n",
        "    if contents:\n",
        "        try:\n",
        "            content = \" \".join(part.get(\"value\", \"\") for part in contents if isinstance(part, Mapping))\n",
        "        except Exception:\n",
        "            content = \"\"\n",
        "    return f\"{title} {summary} {content}\".lower()\n",
        "\n",
        "\n",
        "def _get_content(entry: Mapping[str, object]) -> Optional[str]:\n",
        "    contents = entry.get(\"content\")\n",
        "    if contents:\n",
        "        parts: List[str] = []\n",
        "        for part in contents:\n",
        "            if isinstance(part, Mapping):\n",
        "                value = part.get(\"value\")\n",
        "                if isinstance(value, str):\n",
        "                    parts.append(value)\n",
        "        if parts:\n",
        "            return \"\\n\\n\".join(parts)\n",
        "    summary = entry.get(\"summary\")\n",
        "    return summary if isinstance(summary, str) else None\n",
        "\n",
        "\n",
        "def _parse_published(entry: Mapping[str, object]) -> Optional[datetime]:\n",
        "    published_parsed = entry.get(\"published_parsed\")\n",
        "    if published_parsed:\n",
        "        try:\n",
        "            return datetime(*published_parsed[:6], tzinfo=timezone.utc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    updated = entry.get(\"updated_parsed\")\n",
        "    if updated:\n",
        "        try:\n",
        "            return datetime(*updated[:6], tzinfo=timezone.utc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEC_BASE = \"https://data.sec.gov\"\n",
        "\n",
        "class SECClient:\n",
        "    \"\"\"Helper for querying SEC datasets with the required headers.\"\"\"\n",
        "\n",
        "    TICKER_MAP_PATH = \"/files/company_tickers.json\"\n",
        "\n",
        "    def __init__(self, user_agent: str) -> None:\n",
        "        if not user_agent or \"@\" not in user_agent:\n",
        "            raise ValueError(\"SEC user agent must include a contact email per SEC guidelines\")\n",
        "        self._session = requests.Session()\n",
        "        self._session.headers.update(\n",
        "            {\n",
        "                \"User-Agent\": user_agent,\n",
        "                \"Accept\": \"application/json, text/plain, */*\",\n",
        "                \"Accept-Encoding\": \"gzip, deflate\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def get_json(self, path: str) -> Mapping[str, object]:\n",
        "        response = self._session.get(f\"{SEC_BASE}{path}\", timeout=15)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    @lru_cache(maxsize=1)\n",
        "    def ticker_map(self) -> Dict[str, Dict[str, str]]:\n",
        "        payload = self.get_json(self.TICKER_MAP_PATH)\n",
        "        mapping: Dict[str, Dict[str, str]] = {}\n",
        "        for item in payload.values():\n",
        "            ticker = item.get(\"ticker\")\n",
        "            cik = item.get(\"cik_str\")\n",
        "            title = item.get(\"title\")\n",
        "            if isinstance(ticker, str) and isinstance(cik, int):\n",
        "                mapping[ticker.upper()] = {\n",
        "                    \"cik\": f\"{cik:010d}\",\n",
        "                    \"title\": title or ticker.upper(),\n",
        "                }\n",
        "        return mapping\n",
        "\n",
        "    def resolve_cik(self, query: str) -> Optional[Dict[str, str]]:\n",
        "        normalized = query.strip().upper()\n",
        "        if not normalized:\n",
        "            return None\n",
        "        if normalized.isdigit():\n",
        "            return {\"cik\": normalized.zfill(10), \"title\": normalized}\n",
        "        mapping = self.ticker_map()\n",
        "        if normalized in mapping:\n",
        "            return mapping[normalized]\n",
        "        for entry in mapping.values():\n",
        "            title = entry.get(\"title\", \"\").upper()\n",
        "            if normalized in title:\n",
        "                return entry\n",
        "        return None\n",
        "\n",
        "    def company_submissions(self, cik: str) -> Mapping[str, object]:\n",
        "        return self.get_json(f\"/submissions/CIK{cik}.json\")\n",
        "\n",
        "    def supplemental_submissions(self, filename: str) -> Mapping[str, object]:\n",
        "        return self.get_json(f\"/submissions/{filename}\")\n",
        "\n",
        "    def company_facts(self, cik: str) -> Mapping[str, object]:\n",
        "        return self.get_json(f\"/api/xbrl/companyfacts/CIK{cik}.json\")\n",
        "\n",
        "\n",
        "class SECFilingsProvider(BaseProvider):\n",
        "    \"\"\"Fetches Form 10-K filings from the SEC with financial snippets.\"\"\"\n",
        "\n",
        "    MAX_AGE_YEARS = 10\n",
        "    _FINANCIAL_CONCEPTS: Dict[str, List[str]] = {\n",
        "        \"revenue\": [\n",
        "            \"Revenues\",\n",
        "            \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
        "            \"SalesRevenueNet\",\n",
        "        ],\n",
        "        \"net_income\": [\"NetIncomeLoss\"],\n",
        "        \"assets\": [\"Assets\"],\n",
        "        \"liabilities\": [\"Liabilities\"],\n",
        "        \"cash\": [\n",
        "            \"CashAndCashEquivalentsAtCarryingValue\",\n",
        "            \"CashAndCashEquivalentsPeriodIncreaseDecrease\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def __init__(self, user_agent: str, max_years: int = MAX_AGE_YEARS) -> None:\n",
        "        self._client = SECClient(user_agent)\n",
        "        self._max_years = max(1, max_years)\n",
        "\n",
        "    def fetch(self, query: str, limit: int = 10, **kwargs: Mapping[str, object]) -> Iterable[RawArticle]:\n",
        "        identity = self._client.resolve_cik(query)\n",
        "        if identity is None:\n",
        "            return []\n",
        "        cik = identity[\"cik\"]\n",
        "        company_name = identity.get(\"title\", query.upper())\n",
        "        cutoff = datetime.utcnow() - timedelta(days=365 * self._max_years)\n",
        "        filings = self._collect_filings(cik, cutoff)\n",
        "        if not filings:\n",
        "            return []\n",
        "        facts = self._safe_company_facts(cik)\n",
        "        items: List[RawArticle] = []\n",
        "        for filing in filings[:limit]:\n",
        "            summary = self._compose_summary(filing, facts)\n",
        "            title = f\"{company_name} Form 10-K ({filing['filing_date'].year})\"\n",
        "            source = f\"SEC 10-K FY{filing.get('fy') or filing['filing_date'].year}\"\n",
        "            items.append(\n",
        "                RawArticle(\n",
        "                    title=title,\n",
        "                    url=filing[\"url\"],\n",
        "                    source=source,\n",
        "                    published_at=filing[\"filing_date\"],\n",
        "                    content=summary,\n",
        "                    description=summary,\n",
        "                )\n",
        "            )\n",
        "        return items\n",
        "\n",
        "    def _collect_filings(self, cik: str, cutoff: datetime) -> List[Dict[str, object]]:\n",
        "        aggregated: Dict[str, Dict[str, object]] = {}\n",
        "        datasets = [self._client.company_submissions(cik)]\n",
        "        extra_files = datasets[0].get(\"filings\", {}).get(\"files\", []) if isinstance(datasets[0], Mapping) else []\n",
        "        for extra in extra_files:\n",
        "            name = extra.get(\"name\")\n",
        "            if isinstance(name, str):\n",
        "                datasets.append(self._client.supplemental_submissions(name))\n",
        "        for dataset in datasets:\n",
        "            records = self._iter_records(dataset)\n",
        "            for record in records:\n",
        "                if record.get(\"form\") not in {\"10-K\", \"10-K/A\"}:\n",
        "                    continue\n",
        "                filing_date = self._parse_date(record.get(\"filingDate\"))\n",
        "                if filing_date is None or filing_date < cutoff:\n",
        "                    continue\n",
        "                accession = record.get(\"accessionNumber\")\n",
        "                if not accession or accession in aggregated:\n",
        "                    continue\n",
        "                url = self._build_filing_url(cik, accession, record.get(\"primaryDocument\"))\n",
        "                aggregated[accession] = {\n",
        "                    \"accession\": accession,\n",
        "                    \"filing_date\": filing_date,\n",
        "                    \"report_date\": self._parse_date(record.get(\"reportDate\")),\n",
        "                    \"fy\": record.get(\"fy\"),\n",
        "                    \"form\": record.get(\"form\"),\n",
        "                    \"url\": url,\n",
        "                }\n",
        "        ordered = sorted(aggregated.values(), key=lambda item: item[\"filing_date\"], reverse=True)\n",
        "        return ordered\n",
        "\n",
        "    def _iter_records(self, dataset: Mapping[str, object]) -> List[Dict[str, object]]:\n",
        "        if not isinstance(dataset, Mapping):\n",
        "            return []\n",
        "        if \"filings\" in dataset and isinstance(dataset[\"filings\"], Mapping):\n",
        "            recent = dataset[\"filings\"].get(\"recent\")\n",
        "            if isinstance(recent, Mapping):\n",
        "                return self._normalize_rows(recent)\n",
        "        return self._normalize_rows(dataset)\n",
        "\n",
        "    def _normalize_rows(self, payload: Mapping[str, object]) -> List[Dict[str, object]]:\n",
        "        forms = payload.get(\"form\")\n",
        "        if not isinstance(forms, list):\n",
        "            return []\n",
        "        length = len(forms)\n",
        "        rows: List[Dict[str, object]] = []\n",
        "        for idx in range(length):\n",
        "            row: Dict[str, object] = {}\n",
        "            for key, value in payload.items():\n",
        "                if isinstance(value, list) and len(value) > idx:\n",
        "                    row[key] = value[idx]\n",
        "            rows.append(row)\n",
        "        return rows\n",
        "\n",
        "    def _build_filing_url(self, cik: str, accession: str, primary: Optional[str]) -> str:\n",
        "        stripped_cik = str(int(cik))\n",
        "        accession_no_dashes = accession.replace(\"-\", \"\")\n",
        "        document = primary or \"index.htm\"\n",
        "        return f\"https://www.sec.gov/Archives/edgar/data/{stripped_cik}/{accession_no_dashes}/{document}\"\n",
        "\n",
        "    def _parse_date(self, value: Optional[str]) -> Optional[datetime]:\n",
        "        if not value:\n",
        "            return None\n",
        "        try:\n",
        "            if \"T\" in value:\n",
        "                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\"))\n",
        "            return datetime.strptime(value, \"%Y-%m-%d\")\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    def _safe_company_facts(self, cik: str) -> Mapping[str, object]:\n",
        "        try:\n",
        "            return self._client.company_facts(cik)\n",
        "        except requests.HTTPError:\n",
        "            return {}\n",
        "\n",
        "    def _compose_summary(self, filing: Mapping[str, object], facts: Mapping[str, object]) -> str:\n",
        "        accession = filing.get(\"accession\")\n",
        "        fy = filing.get(\"fy\")\n",
        "        report_date = filing.get(\"report_date\")\n",
        "        metrics = self._extract_financials(accession, facts)\n",
        "        parts = [f\"Form {filing.get('form')} filed on {filing['filing_date'].date()}.\"]\n",
        "        if fy:\n",
        "            parts.append(f\"Fiscal year: {fy}.\")\n",
        "        if report_date:\n",
        "            parts.append(f\"Period end: {report_date.date()}.\")\n",
        "        if metrics:\n",
        "            formatted = \", \".join(\n",
        "                f\"{label}: {value}\" for label, value in metrics.items()\n",
        "            )\n",
        "            parts.append(formatted)\n",
        "        else:\n",
        "            parts.append(\"Financial highlights unavailable from XBRL dataset.\")\n",
        "        return \" \".join(parts)\n",
        "\n",
        "    def _extract_financials(self, accession: Optional[str], facts: Mapping[str, object]) -> Dict[str, str]:\n",
        "        if not accession or not isinstance(facts, Mapping):\n",
        "            return {}\n",
        "        fact_root = facts.get(\"facts\")\n",
        "        if not isinstance(fact_root, Mapping):\n",
        "            return {}\n",
        "        us_gaap = fact_root.get(\"us-gaap\")\n",
        "        if not isinstance(us_gaap, Mapping):\n",
        "            return {}\n",
        "        results: Dict[str, str] = {}\n",
        "        for label, concepts in self._FINANCIAL_CONCEPTS.items():\n",
        "            value = self._find_fact_value(us_gaap, concepts, accession)\n",
        "            if value is not None:\n",
        "                results[self._label_for(label)] = self._format_currency(value)\n",
        "        return results\n",
        "\n",
        "    def _find_fact_value(self, us_gaap: Mapping[str, object], concepts: List[str], accession: str) -> Optional[float]:\n",
        "        for concept in concepts:\n",
        "            concept_payload = us_gaap.get(concept)\n",
        "            if not isinstance(concept_payload, Mapping):\n",
        "                continue\n",
        "            units = concept_payload.get(\"units\")\n",
        "            if not isinstance(units, Mapping):\n",
        "                continue\n",
        "            for series in units.values():\n",
        "                if not isinstance(series, list):\n",
        "                    continue\n",
        "                for entry in series:\n",
        "                    if not isinstance(entry, Mapping):\n",
        "                        continue\n",
        "                    if entry.get(\"accn\") == accession and entry.get(\"form\") in {\"10-K\", \"10-K/A\"}:\n",
        "                        val = entry.get(\"val\")\n",
        "                        if isinstance(val, (int, float)):\n",
        "                            return float(val)\n",
        "        return None\n",
        "\n",
        "    def _label_for(self, key: str) -> str:\n",
        "        mapping = {\n",
        "            \"revenue\": \"Revenue\",\n",
        "            \"net_income\": \"Net income\",\n",
        "            \"assets\": \"Total assets\",\n",
        "            \"liabilities\": \"Total liabilities\",\n",
        "            \"cash\": \"Cash & cash equivalents\",\n",
        "        }\n",
        "        return mapping.get(key, key.title())\n",
        "\n",
        "    def _format_currency(self, value: float) -> str:\n",
        "        suffixes = [\n",
        "            (1e12, \"T\"),\n",
        "            (1e9, \"B\"),\n",
        "            (1e6, \"M\"),\n",
        "        ]\n",
        "        sign = \"-\" if value < 0 else \"\"\n",
        "        magnitude = abs(value)\n",
        "        for threshold, suffix in suffixes:\n",
        "            if magnitude >= threshold:\n",
        "                return f\"{sign}${magnitude / threshold:.2f}{suffix}\"\n",
        "        return f\"{sign}${magnitude:,.0f}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockProvider(BaseProvider):\n",
        "    \"\"\"Returns hard-coded articles for offline development.\"\"\"\n",
        "\n",
        "    def fetch(self, query: str, limit: int = 10, **kwargs) -> Iterable[RawArticle]:\n",
        "        now = datetime.utcnow()\n",
        "        sample = [\n",
        "            RawArticle(\n",
        "                title=f\"{query.title()} expands sustainability efforts\",\n",
        "                url=\"https://example.com/sustainability\",\n",
        "                source=\"Example News\",\n",
        "                published_at=now - timedelta(hours=2),\n",
        "                content=(\n",
        "                    f\"{query} announced new sustainability targets aimed at reducing emissions by 30% \"\n",
        "                    \"over the next five years. The initiative includes investments in renewable energy \"\n",
        "                    \"and supply chain transparency.\"\n",
        "                ),\n",
        "                description=\"Company targets lower emissions and greener supply chains.\",\n",
        "            ),\n",
        "            RawArticle(\n",
        "                title=f\"Analysts debate {query} quarterly earnings\",\n",
        "                url=\"https://example.com/earnings\",\n",
        "                source=\"Market Watchers\",\n",
        "                published_at=now - timedelta(days=1),\n",
        "                content=(\n",
        "                    f\"Market analysts offered mixed reactions to {query}'s latest earnings report, citing \"\n",
        "                    \"flat revenue growth but improving operating margins. Investor sentiment appears \"\n",
        "                    \"cautious heading into the next quarter.\"\n",
        "                ),\n",
        "                description=\"Mixed analyst sentiment following the latest results.\",\n",
        "            ),\n",
        "        ]\n",
        "        return sample[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d51075e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsAgent:\n",
        "    \"\"\"Aggregates, summarizes, and scores news articles.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[AgentConfig] = None, providers: Optional[Iterable[BaseProvider]] = None) -> None:\n",
        "        self.config = config or AgentConfig.from_env()\n",
        "        if providers is not None:\n",
        "            self.providers = list(providers)\n",
        "        else:\n",
        "            self.providers = self._build_providers()\n",
        "        if not self.providers:\n",
        "            raise RuntimeError(\"No providers configured for NewsAgent\")\n",
        "\n",
        "    def _build_providers(self) -> List[BaseProvider]:\n",
        "        providers: List[BaseProvider] = []\n",
        "        if getattr(self.config, \"newsapi_key\", None):\n",
        "            try:\n",
        "                providers.append(NewsAPIProvider(self.config.newsapi_key))\n",
        "            except Exception as exc:\n",
        "                print(f\"Skipping NewsAPI provider: {exc}\")\n",
        "        providers.append(WiredRSSProvider())\n",
        "        providers.append(MockProvider())\n",
        "        return providers\n",
        "\n",
        "    def search(self, query: str, limit: Optional[int] = None, **kwargs) -> List[NewsItem]:\n",
        "        if not query or not query.strip():\n",
        "            raise ValueError(\"Query must be provided\")\n",
        "        limit = limit or self.config.default_limit\n",
        "        seen_urls: set[str] = set()\n",
        "        seen_titles: set[str] = set()\n",
        "        seen_sources: dict[str, int] = {}\n",
        "        results: List[NewsItem] = []\n",
        "        for provider in self.providers:\n",
        "            for raw in provider.fetch(query=query, limit=limit, **kwargs):\n",
        "                if raw.url:\n",
        "                    if raw.url in seen_urls:\n",
        "                        continue\n",
        "                    if not self._is_allowed_domain(raw.url):\n",
        "                        continue\n",
        "                    seen_urls.add(raw.url)\n",
        "                dedupe_key = self._dedupe_key(raw)\n",
        "                if dedupe_key and dedupe_key in seen_titles:\n",
        "                    continue\n",
        "                source_key = self._source_key(raw)\n",
        "                if source_key and self._exceeds_source_limit(source_key, seen_sources):\n",
        "                    continue\n",
        "                item = self._process(raw)\n",
        "                results.append(item)\n",
        "                if dedupe_key:\n",
        "                    seen_titles.add(dedupe_key)\n",
        "                if source_key:\n",
        "                    seen_sources[source_key] = seen_sources.get(source_key, 0) + 1\n",
        "                if len(results) >= limit:\n",
        "                    return results\n",
        "        return results\n",
        "\n",
        "    def _process(self, article: RawArticle) -> NewsItem:\n",
        "        text = article.content or article.description\n",
        "        summary = summarize(text)\n",
        "        sentiment_label, sentiment_score = score_sentiment(text or \"\")\n",
        "        excerpt = article.description or article.content\n",
        "        if excerpt and len(excerpt) > 280:\n",
        "            excerpt = excerpt[:277].rstrip() + \"...\"\n",
        "        return NewsItem(\n",
        "            title=article.title,\n",
        "            url=article.url,\n",
        "            source=article.source,\n",
        "            published_at=article.published_at,\n",
        "            summary=summary,\n",
        "            sentiment=sentiment_label,\n",
        "            sentiment_score=sentiment_score,\n",
        "            excerpt=excerpt,\n",
        "        )\n",
        "\n",
        "    def _is_allowed_domain(self, url: str) -> bool:\n",
        "        if not self.config.allowed_domains:\n",
        "            return True\n",
        "        parsed = urlparse(url)\n",
        "        if not parsed.netloc:\n",
        "            return True\n",
        "        hostname = parsed.netloc.lower()\n",
        "        for domain in self.config.allowed_domains:\n",
        "            domain = domain.lower()\n",
        "            if hostname == domain or hostname.endswith(f\".{domain}\"):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def to_dict(self, item: NewsItem) -> dict:\n",
        "        data = asdict(item)\n",
        "        if item.published_at is not None:\n",
        "            data[\"published_at\"] = item.published_at.isoformat()\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = AgentConfig.from_env()\n",
        "agent = NewsAgent(config=config)\n",
        "agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_items = agent.search(\"INFY\", limit=3)\n",
        "for idx, item in enumerate(sample_items, start=1):\n",
        "    print(f\"[{idx}] {item.title} - {item.sentiment} ({item.sentiment_score:.2f})\")\n",
        "    print(f\"    Source: {item.source}\")\n",
        "    if item.summary:\n",
        "        print(f\"    Summary: {item.summary}\")\n",
        "    elif item.excerpt:\n",
        "        print(f\"    Excerpt: {item.excerpt}\")\n",
        "    if item.url:\n",
        "        print(f\"    URL: {item.url}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Flask App Snippet\n",
        "\n",
        "You can adapt the snippet below to run the Flask API inside the notebook by removing the guard and running the cell. It will block the kernel, so it's typically better to keep using `app.py` for serving requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from flask import Flask, jsonify, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "_agent = agent\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def healthcheck():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "\n",
        "@app.post(\"/news\")\n",
        "def fetch_news():\n",
        "    payload = request.get_json(silent=True) or {}\n",
        "    query = payload.get(\"query\")\n",
        "    limit = payload.get(\"limit\")\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"`query` is required\"}), 400\n",
        "    try:\n",
        "        items = _agent.search(query=query, limit=limit)\n",
        "        return jsonify([_agent.to_dict(item) for item in items])\n",
        "    except ValueError as exc:\n",
        "        return jsonify({\"error\": str(exc)}), 400\n",
        "\n",
        "# To run inside notebook (blocks execution):\n",
        "# app.run(host=\"0.0.0.0\", port=8008, debug=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}